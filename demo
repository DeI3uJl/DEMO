GIT вау
1устанавливаем: apt-get install git
2создайте папку типо: mkdir /xyu и переходим туда
3пишем: git clone https://github.com/yukopo/DEMO.git
все, дальше просто копируете конфиги куда надо
например: cp /xyu/DEMO/dhcpd.conf /etc/dhcp/dhcpd.conf
или cp /xyu/DEMO/nftables.conf /etc/nftables.conf
cp /xyu/DEMO/db.4.4.4 /etc/bind/db.4.4.4


для удобства сделайте все это на графике, и по ссш копируйте файлы на другие компы
например: scp /xyu/DEMO/db.4.4.4 root@192.168.200.3:/etc/bind изи


чтобы коммитить изменения в репозиторий понадобиться:
проделать первые 3три пункта, перейти в папку демо(так как она является репозиторием гит) и вписать следуещее:
git config --global user.email "you@example.com"
git config --global user.name "Ваше Имя"
(входим в СВОю учетку гита)
git add -A (добавляем в репозиторий ВСЕ наши изменения, если определенные то перечислите их названия вместо флага А)
git commit -m "ваше сообщение" (добавляет “описание” для ваших изменений, напишите там че вы сделали)
git push origin (пушим наши изменения в сеть, он попросит ввести имя либо репозитория, то есть мое - yuokopo, либо имя вашего гитхаба, и придется ввести пароль вот такой: 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           лучше делать конечно там где есть графика и просто вставлять, но для дома вместо vnc можете использовать такой прикол как mobaXterm, загуглите, гайд писать не буду, разберетесь)
все, изменения теперь в сети интернет


!!и раз мы не используем форки, как нормальные люди, а сохраняем все в главный отдел, следим за тем, что добавляем!!
Задание 1 модуля 1
Не забываем делать apt-get update и apt-get upgrade
Переходить в рута через SU -
 
1. Выполните базовую настройку всех устройств: 
a.	Присвоить имена в соответствии с топологией
Для установки имени виртуальной машины необходимо воспользоваться утилитой HOSTNAMECTL = hostnamectl set-hostname vashe_imya ; exec bash и reboot
b.	Рассчитайте IP-адресацию IPv4 и IPv6. Необходимо заполнить таблицу №1, чтобы эксперты могли проверить ваше рабочее место. 
c.	Пул адресов для сети офиса BRANCH - не более 16 
Подойдёт маска /28 (255.255.255.240), всего хостов 16 – 1 = 15.
d.	Пул адресов для сети офиса HQ - не более 64 
Подойдёт маска /26 (255.255.255.192), всего хостов 64 – 1 = 63.
Таблица №1 
Имя устройства
IP
CLI
e0: 3.3.3.10/24
ISP
e0: 5.5.5.1/24
e1: 4.4.4.1/24
e2: 3.3.3.1/24
HQ-R
e0: 4.4.4.100/24
e1: 192.168.200.1/26
HQ-SRV
e0: DHCPv4 (выделенный IP 192.168.200.3/26)
BR-R
e0: 5.5.5.100
e1: 172.16.100.1/28
BR-SRV
e0: 172.16.100.2/28

2. Настройте внутреннюю динамическую маршрутизацию по средствам FRR. Выберите и обоснуйте выбор протокола динамической маршрутизации из расчёта, что в дальнейшем сеть будет масштабироваться. 
a.	Составьте топологию сети L3.
echo “net.ipv4.ip_forward=1” > /etc/sysctl.conf; sysctl –p вкл. пересылку пакетов 
либо nano /etc/sysctl.conf, и раскоментить строку net.ipv4.ip_forward=1. При reboot может сломаться и надо прописать  sysctl -p.
На ISP, HQ-R, BR-R устанавливаем FRR apt install frr, после редачим файл nano /etc/frr/daemons и включаем ospf
 
systemctl restart frr

 
Можно прописать default-information originate для маршрута по умолчанию и сохранить  write.

3. Настройте автоматическое распределение IP-адресов на роутере HQ-R.
apt install isc-dhcp-server. Зададим интерфейс на котором будет работать DHCP-сервер отредактировав файл nano /etc/default/isc-dhcp-server и придав значение INTERFACESv4="ens…" 

После этого настроим сам DHCP-server, nano /etc/dhcp/dhcpd.conf и добавляем следующие строчки:  меняя айпи на свои
 
a.	Учтите, что у сервера должен быть зарезервирован адрес. 
Для этого нужно узнать MAC-адрес интерфейса HQ-SRV, подключенного к HQ-R командой 
ip -br link 
 









Затем добавляем в /etc/dhcp/dhcpd.conf следующие строки:

перезапускаем isc-dhcp-server

4. Настройте локальные учетные записи на всех устройствах в соответствии с таблицей 2. 
Таблица №2 

Учётная запись
Пароль
Примечание
Admin
P@ssw0rd
CLI HQ-SRV HQ-R
Branch admin
P@ssw0rd
BR-SRV BR-R
Network admin
P@ssw0rd
HQ-R BR-R BR-SRV

adduser <имя_пользователя> --force-badname
Также установим на каждое устройство пакет sudo и добавим в группу sudo свежесозданных пользователей, это нам понадобится в дальнейшем:
apt install sudo
usermod -aG sudo <имя_пользователя>
5. Измерьте пропускную способность сети между двумя узлами HQ-R-ISP по средствам утилиты iperf 3. Предоставьте описание пропускной способности канала со скриншотами. 
Apt install iperf3
Запуск сервера iperf3 -s
На клиенте пишем iperf3 -c <ip_адрес>
6. Составьте backup скрипты для сохранения конфигурации сетевых устройств, а именно HQ-R BR-R. Продемонстрируйте их работу. 
-

7. Настройте подключение по SSH для удаленного конфигурирования устройства HQ-SRV по порту 2222. Учтите, что вам необходимо перенаправить трафик на этот порт посредством контролирования трафика. 
На HQ-R в /etc/nftables.conf прописываем 				(измените ip на ваш)
 
#1– открыть порт 2222;          	можно не писать, не уверен
#2– перенаправление трафика с порта 2222 на адрес 192.168…   порт 22
Применяем правила командой nft -f /etc/nftables.conf
хз где на HQ-R или HQ-SRV, а может и везде где нужен ssh по root надо изменить 2 строки в файле nano /etc/ssh/sshd_config
 permitrootlogin yes
 pubkeyauthentication yes
8. Настройте контроль доступа до HQ-SRV по SSH со всех устройств, кроме CLI. 
В nano /etc/ssh/sshd_config добавить две строчки на HQ-SRV:
Match Address <ip_адрес> # IP-адрес CLI
DenyUsers *
 
Перезапускаем SSH systemctl restart ssh
Модуль 2: Организация сетевого администрирования
1. Настройте DNS-сервер на сервере HQ-SRV: 
a.	На DNS сервере необходимо настроить 2 зоны 
Зона hq.work, также не забудьте настроить обратную зону.
Имя
Тип записи
Адрес
hq-r.hq.work
A, PTR
IP-адрес
hq-srv.hq.work	
A, PTR	
IP-адрес

Зона branch.work
Имя
Тип записи
Адрес
hq-r.hq.work
A, PTR
IP-адрес
br-srv.branch.work
A
IP-адрес

Установим пакет DNS-сервера на HQ-SRV apt install bind9 и затем создадим файлики прямых и обратных зон в /etc/bind/, взяв за основу файл db.local:
cd /etc/bind
cp db.local db.hq.work # прямая зона hq.work
cp db.local db.branch.work # прямая зона branch.work		
cp db.local db.192.168.200 # обратная зона для сети 192.168.200.0	
cp db.local db.4.4.4 # обратная зона для сети 4.4.4.0
cp db.local db.5.5.5 # обратная зона для сети 5.5.5.0
 
Конфигурация /etc/bind/db.hq.work

Конфигурация /etc/bind/db.branch.work

Конфигурация /etc/bind/db.192.168.200

Конфигурация /etc/bind/db.4.4.4

Конфигурация /etc/bind/db.5.5.5
Затем нам нужно указать все эти зоны в файле nano /etc/bind/named.conf.default-zones  


Конфигурация /etc/bind/named.conf.default-zones
После этого перезагружаем службу systemctl restart bind9.service. Проверяем командой nslookup <имя/ip_адрес> <ip_адрес_dns_сервера>

nslookup посланный с HQ-R на HQ-SRV
Дополнительно можно сделать так, чтобы устройства автоматически использовали HQ-SRV как DNS-сервер. Для этого редактируем nano /etc/resolv.conf и добавляем строчку 
nameserver <ip_адрес_dns_сервера>

2. Настройте синхронизацию времени между сетевыми устройствами по протоколу NTP. 
a.	В качестве сервера должен выступать роутер HQ-R со стратумом 5
b.	Используйте Loopback интерфейс на HQ-R, как источник сервера времени 
c.	Все остальные устройства и сервера должны синхронизировать свое время с роутером HQ-R 
d.	Все устройства и сервера настроены на московский часовой пояс (UTC +3) 
на ВСЕ устройства установить пакет NTP apt install chrony
a.	В качестве сервера должен выступать роутер HQ-R со стратумом 5
Отредактируем файл настройки на HQ-R nano /etc/chrony/chrony.conf, в нём должны быть следующие строки:

Конфигурация /etc/chrony/chrony.conf на HQ-R
Не забудьте перезапустить службу systemctl restart chrony.service.

Chrony автоматически использует Loopback интерфейс как источник сервера времени
c. Все остальные устройства и сервера должны синхронизировать свое время с роутером HQ-R
На всех остальных устройствах редактируем nano /etc/chrony/chrony.conf и пишем следующие строчки:
server <ip_адрес> iburst 	# IP-адрес который мы сделали на NTP-сервере

Не забудьте перезапустить службу systemctl restart chrony.service. Проверить настройку можно командой chronyc sources на любом из клиентов и сверить столбик Stratum 

Успешное синхронизирование времени клиента с NTP-сервером
Не забудьте, что у BR-SRV нет доступа в белую сеть, поэтому чтобы он тоже синхронизировал время с HQ-R нужно сделать трансляцию IP-адресов на BR-R.  
поэтому в /etc/nftables.conf добавим нат маскарадинг

table ip nat {} — создание таблицы с названием nat. Использование протокола IPv4; 
chain postrouting {} — создание цепочки с названием postrouting; 
type nat — тип цепочки=nat; 
hook postrouting — только трафик, прошедший процесс маршрутизации, попадает в эту цепочку; 
priority 0 — цепочка выполняется самой первой для трафика; 
ip saddr — указывает подсеть, трафик из которой должен попасть в nat; 
oifname — имя интерфейса, на который был смаршрутизирован трафик; 
counter — ключевое слово для подсчета пакетов, попавших в данное правило; 
masquerade — действие, производимое с трафиком (трансляция в адрес исходящего интерфейса).
Для проверки корректности написания и применения выполните nft -f /etc/nftables.conf
d.	Все устройства и сервера настроены на московский часовой пояс (UTC +3)
Для этого на всех устройствах прописываем timedatectl set-timezone Europe/Moscow. Настройку можно проверить командой timedatectl
 
Успешная настройка часового пояса на устройствах

3. Настройте сервер домена выбор, его типа обоснуйте, на базе HQ-SRV через web интерфейс, выбор технологий обоснуйте. 
a.	Введите машины BR-SRV и CLI в данный домен 
b.	Организуйте отслеживание подключения к домену 
Сначала настроим сервер OpenLDAP на HQ-SRV. Для начала нам нужно проверить настройки имени хоста и его FQDN.

/etc/hosts на HQ-SRV
 
Проверка FQDN

Далее устанавливаем необходимые пакеты для web-менеджмента:
apt install apache2 php php-cgi libapache2-mod-php php-mbstring php-common php-pear
apt install slapd ldap-utils
вылезет менюшка. Придумываем пароль для админа LDAP, в моём случае будет P@ssw0rd

 
Проверяем настройку командой slapcat   (HQ-SRV)


После этого устанавливаем Диспетчер учетных записей LDAP 
apt install ldap-account-manager

После того как установили пакет, включаем PHP расширение PHP-CGI командой 
a2enconf php*-cgi и после этого перезагружаем службу сервера Apache и делаем так, чтобы она загружалась вместе с системой:
systemctl restart apache2.service
systemctl enable apache2.service

Теперь мы можем зайти в веб-интерфейс LAM по адресу «http://<ip_адрес>/lam». Сразу начнём конфигурацию, заходим во вкладку LAM configuration



Затем заходим в Edit server profiles

Вводим пароль, по умолчанию это lam, и нажимаем Ok (рис. 38)
 
Меняем часовой пояс на московский

Листаем вниз и находим Tool settings. В строчке Tree suffix меняем на dc=hq,dc=work

Под Tool settings располагается Security settings. Меняем строчку List of valid users на cn=admin,dc=hq,dc=work

Листаем ниже и находим Profile password. Задаем пароль по умолчанию для веб-профиля admin, в моём случае это будет P@ssw0rd


Сохраняем настройки кнопкой Save 
 	
Теперь обратно в настройки и вводим пароль P@ssw0rd так как пароль поменялся и у lam
 





Заходим во вкладку Account types

Меняем в Users и Group строчки с LDAP suffix 

Сохраняем настройки кнопкой Save.
Теперь входим в основной веб-интерфейс LAM, пароль также P@ssw0rd

Попросит создать подразделения, нажимаем Create
	
Вкладка Tools -> OU editor

Видим New organizational unit, нажимаем на Parent DN и выбираем People > hq > work
В поле Name пишем BR-SRV  и нажимаем Ok.


Вкладка Tools -> Tree view, затем вкладка Accounts -> Groups


Нам нужно создать новую группу. Нажимаем на кнопку New group 

В строчку Group name пишем grp-BR-SRV
 
Далее вкладка Accounts -> Users. Нам нужно создать пользователей для BR-SRV, значит создаём Branch_admin и Network_admin. Для этого нажимаем на кнопку New user


Появляется менюшка создания пользователя. Во вкладке Personal в строку Last name вписываем имя пользователя  Во вкладке Unix исправляем строчку User name. Затем задаём пароль пользователю, нажимаем на кнопку Set password, пишем пароль и нажимаем Ok



Сохраняем кнопкой Save. Проделываем данные шаги для обоих пользователей.

Успешно создали пользователей

Настройка сервера OpenLDAP на этом закончена. 

a.	Введите машины BR-SRV и CLI в данный домен
Теперь перейдём к настройке клиента (в данном случае BR-SRV). Также проверяем имя хоста и FQDN 

/etc/hosts на BR-SRV
 

Проверка FQDN

Устанавливаем нужные пакеты для клиента OpenLDAP 
apt install libnss-ldap libpam-ldap ldap-utils nscd. 
При установке сразу вылезет менюшка настройки libnss-ldap, вписываем ldap://<ip_адрес>  Возможно нужно писать порт ldap://192.168.200.3:389 


Далее вводим dc=hq,dc=work

Выбираем 3 версию LDAP
	
Пишем аккаунт админа cn=admin,dc=hq,dc=work

Вводим пароль админа LDAP, в моём случае P@ssw0rd

Будет жаловаться на ненастроенный nsswitch.conf, нажимаем Ok

Теперь настраиваем libpam-ldap. 
Allow LDAP admin account to behave like local root -> Yes 

Does the LDAP database require login -> No
 
Вписываем cn=admin,dc=hq,dc=work

Вписываем пароль администратора, в моём случае P@ssw0rd


Теперь редактируем файлы, в nano /etc/nsswitch.conf надо изменить строчки
passwd: compat systemd ldap
group:	compat systemd ldap
shadow: compat
 
Конфигурация /etc/nsswitch.conf
Теперь в nano /etc/pam.d/common-password из строчки
Password [success=1 user_unknown=ignore default=die] pam_ldap.so use_authtok try_first_pass удаляем use_authtok
 
Конфигурация /etc/pam.d/common-password

В nano /etc/pam.d/common-session добавляем в конец файла 
session optional pam_mkhomedir.so skel=/etc/skel umask=077 

Конфигурация /etc/pam.d/common-session
Перезагружаем и ставим на запуск при загрузке службу nscd:
systemctl restart nscd.service
systemctl enable nscd.service




b.	Организуйте отслеживание подключения к домену
Проверить подключение к домену можно командой 
ldapsearch -x -H ldap://<ip_адрес> -b "dc=hq,dc=work" 

Команда ldapsearch

После этого удаляем созданных в 1 модуле пользователей командой deluser <имя_пользователя> и удаляем их домашние папки rm –rf /home/<имя_пользователя>, так как LDAP создаст их за нас. Перезагружаем устройство и входим под любым пользователей
 
Вход под пользователем LDAP
4. Реализуйте файловый SMB или NFS (выбор обоснуйте) сервер на базе сервера HQ-SRV. 
a.	Должны быть опубликованы общие папки по названиям: 
   i. Branch_Files - только для пользователя Branch admin; 
   ii. Network - только для пользователя Network admin; 
   iii. Admin_Files - только для пользователя Admin; 
b.	Каждая папка должна монтироваться на всех серверах в папку /mnt/ (например, /mnt/All_files) автоматически при входе доменного пользователя в систему и отключаться при его выходе из сессии. Монтироваться должны только доступные пользователю каталоги. 
Установим на HQ-SRV пакет для работы с SMB apt install samba. Выбор пал на SMB, так как он совместим с большим количеством систем, а также очень гибок в настройке, а также позволяет определять пользователей, имеющих доступ к той или иной сетевой папке, что кстати пригодится при выполнении следующего пункта. 
a.	Должны быть опубликованы общие папки по названиям: 
i.	Branch_Files - только для пользователя Branch admin; 
ii.	Network - только для пользователя Network admin; 
iii.	Admin_Files - только для пользователя Admin;
Создадим в корни папку share командой mkdir /share, где будут лежать все общие папки пользователей mkdir /share/Branch_Files, mkdir /share/Network, mkdir /share/Admin_Files. Имейте в виду, что на HQ-SRV для работы общего ресурса /share/Branch_Files придется создать еще одного пользователя с именем Branch_admin. Теперь задаём пароль для всех пользователей командой smbpasswd -a <имя_пользователя>. Далее редактируем nano /etc/samba/smb.conf и пишем следующие строки для трёх созданных директорий:
Конфигурация /etc/samba/smb.conf

Не забудьте перезагрузить службу systemctl restart smbd.service.
После этого на любом клиенте можно проверить работоспособность командой 
smbclient //<ip_адрес>/<имя_общего_ресурса> --user <имя_пользователя>. 
Если команды нет, устанавливаем apt install smbclient.

Успешное подключение к SMB ресурсу

b.	Каждая папка должна монтироваться на всех серверах в папку /mnt/ (например, /mnt/All_files) автоматически при входе доменного пользователя в систему и отключаться при его выходе из сессии. Монтироваться должны только доступные пользователю каталоги.
Для монтирования общих ресурсов при входе в систему нам потребуется установить пакет CIFS apt install cifs-utils. Теперь нам нужно понять какие файлы выполняются при входе и выходе пользователя из системы. По стандарту с домашней папке каждого пользователя существуют 2 файла .profile (срабатывает при входе в систему) и .bash_logout (срабатывает при выходе из системы). К сожалению, для монтирования удаленных ресурсов нужны соответствующие права, но к счастью мы дали права администратора всем пользователям в 1 модуле 4 задании. Поэтому нам достаточно вписать нужные команды в конец этих файлов чтобы выполнить эти задания. 



Рассмотрим на примере пользователя Admin:
mkdir /mnt/Admin_Files
cd /home/Admin
nano .profile
Добавляем в конец строчку (это всё одна строчка):
echo "P@ssw0rd" | sudo -S mount -t cifs //192.168.200.3/Admin_files /mnt/Admin_files -o credentials=/home/Admin/.smbcredentials,uid=Admin,gid=Admin
Сохраняем
nano .smbcredentials
Пишем в файл:
username=Admin
password=P@ssw0rd
chmod 600 .smbcredentials
nano .bash_logout
Добавляем следующую строку в конец:
echo "P@ssw0rd" | sudo -S umount /mnt/Admin_files
Готово, теперь можно создать на HQ-SRV какой-нибудь файл touch /share/Admin_Files/test, зайти под пользователем Admin на клиенте и прописать dir /mnt/Admin_Files. Проделайте аналогично для всех пользователей на всех машинах. 
На машинах с графическим интерфейсом это может делаться по другому!!! Если не работает, посмотрите какой у вас пакет на графический интерфейс (XTERM, GNOME, LightDM и т.д.) и посмотрите какие файлы/настройки запускаются вместе с заходом/выходом пользователя!!!



5. Сконфигурируйте веб-сервер LMS Apache на сервере BR-SRV: 
a.	На главной странице должен отражаться номер места 
b.	Используйте базу данных mySQL 
c.	Создайте пользователей в соответствии с таблицей, пароли у всех пользователей «P@ssw0rd» 
Пользователь Группа Admin Admin Manager1 Manager Manager2 Manager Manager3 Manager User1 WS User2 WS User3 WS 32 User4 WS User5 TEAM User6 TEAM User7 TEAM 
------------)))
6. Запустите сервис MediaWiki используя docker на сервере HQ-SRV. 
a.	Установите Docker и Docker Compose. 
b.	Создайте в домашней директории пользователя файл wiki.yml для приложения MediaWiki:
i. Средствами docker compose должен создаваться стек контейнеров с приложением MediaWiki и базой данных
ii. Используйте два сервиса; 
iii. Основной контейнер MediaWiki должен называться wiki и использовать образ mediawiki; 
iv. Файл LocalSettings.php с корректными настройками должен находиться в домашней папке пользователя и автоматически монтироваться в образ; 
v. Контейнер с базой данных должен называться db и использовать образ mysql; 
vi. Он должен создавать базу с названием mediawiki, доступную по стандартному порту, для пользователя wiki с паролем DEP@ssw0rd; 
vii. База должна храниться в отдельном volume с названием dbvolume. MediaWiki должна быть доступна извне через порт 8080. 
Для начала установим нужные пакеты для работы MediaWiki 
apt install docker.io docker-compose. 
Создадим отдельную директорию для MediaWiki и перейдём в неё mkdir /root/mediawiki, cd /root/mediawiki. Затем создадим новый файл nano docker-compose.yml и впишем следующее:Прошу заметить, что пробелы играют большую роль!!!!

Конфигурация docker-compose.yml
Теперь выполним команду docker-compose up -d, чтобы запустить контейнеры. После выполнения всех этих шагов, Docker и Docker Compose будут установлены, а контейнеры Mediawiki и MariaDB будут запущены. Для того чтобы проверить, переходим в веб-браузере по адресу http://localhost:8080. Видим страницу с надписью MediaWiki и то что файл LocalSetings.php не найден. Переходим к настройке MediaWiki, нажимаем на set up the wiki

Страница MediaWiki


Выбираем русский язык

Листаем вниз и нажимаем «Далее» 

Далее указываем то, что писали в файле docker-compose.yml. В данном случае под «Имя базы данных» пишем mediawiki, под «Имя пользователя базы данных» пишем wiki, а под «Пароль базы данных» пишем P@ssw0rd (рис. 89).

WIKI NOT WORKING

медиавики докер 
apt install docker.io docker-compose
docker pull qweui/mariadb123
docker pull qweui/mediawiki123
apt install mariadb-server
docker run qweui/mariadb123
docker run qweui/mediawiki123 -p 8080:80
mysql -h 172.17.0.2 -u root -p
create database medawiki;
create user wiki identified by ‘P@ssw0rd’;
grant all privileges on medawiki.* to wiki;
все
http://localhost:8080

Модуль 3: Эксплуатация объектов сетевой инфраструктуры
1. Реализуйте мониторинг по средствам rsyslog на всех Linux хостах. 
a.	Составьте отчёт о том, как работает мониторинг 
Установим на все Linux хосты утилиту мониторинга apt install rsyslog. сервером rsyslog у нас будет HQ-SRV, а все остальные устройства будут его клиентами. На сервере редактируем файл nano/etc/rsyslog.conf. 
Конфигурация /etc/rsyslog.conf на HQ-SRV

После чего нужно перезагрузить службу systemctl restart rsyslog.service
На клиентах же всё просто, в конец nano /etc/rsyslog.conf надо добавить строчку: *.*@<ip_адрес сервера>

Конфигурация /etc/rsyslog.conf на клиентах
Проверяем при помощи отправки тестового сообщения с клиента на сервер командой logger -p mail.err test. После чего на сервере должны создаться папки с названиями машин в директории ls /opt/logs 

Проверка настройки системы мониторинга rsyslog
2. Выполните настройку центра сертификации на базе HQ-SRV: 
a.	Выдайте сертификаты для SSH; 
b.	Выдайте сертификаты для веб серверов; 
Для начала создадим отдельную директорию для сертификатов mkdir /root/cert и перейдем в неё cd /root/cert. Теперь нам нужно сгенерировать ключи Центра Сертификации (Certificate Authority) командой
ssh-keygen -t rsa -b 4096 -f host_ca -C host_ca.
Вводим кодовую фразу, в моём случае пускай будет P@ssw0rd
Создание ключей для центра сертификации
При успешной генерации должны были сгенерироваться 2 файла: host_ca и host_ca.pub. 
Данным образом мы сгенерировали приватный (host_ca) и публичный (host_ca.pub) ключи, с помощью которых мы будем подписывать сертификаты для хостов. Нам также нужно создать ключи пользователей, что делается аналогично командой ssh-keygen -t rsa -b 4096 -f user_ca -C user_ca

Создание ключей для пользователей

Теперь в директории у нас находится 4 файла с ключами. Теперь произведем выдачу сертификатов хоста для аутентификации хостов для пользователей. Для этого создадим новый ключ хоста, подписанный ключом центра сертификации:
ssh-keygen -f ssh_host_rsa_key -N '' -b 4096 -t rsa
ssh-keygen -s host_ca -I <имя сертификата> -h -n <FQDN хоста(ов) через запятую> -V +<время> ssh_host_rsa_key.pub
Пример создания ключа на год для BR-SRV на рис. 95.
Создание ключа для BR-SRV

Данным образом мы сгенерировали 3 файла для BR-SRV: ssh_host_rsa_key, ssh_host_rsa_key.pub и ssh_host_rsa_key-cert.pub. Теперь их надо скопировать на сервер BR-SRV командой scp ssh_host_rsa_key* root@<ip_адрес>:/etc/ssh (Если не получается, проверьте что на удалённом сервере разрешен вход от пользователя root). Теперь перейдём к настройке удаленной машины, в моём случае BR-SRV.
Отредактируем файл nano /etc/ssh/sshd_config, вписав в конец следующую настройку:
HostCertificate /etc/ssh/ssh_host_rsa_key-cert.pub

Конфигурация /etc/ssh/sshd_config на BR-SRV

Теперь BR-SRV настроен на предоставление сертификата любому, кто подключается. Чтобы ssh клиент смог воспользоваться этим и автоматически довериться хосту на основе удостоверения сертификата, необходимо добавить открытый ключ CA в ваш файл known_hosts. Сделаем это на примере HQ-SRV.

Удаляем предыдущий файл rm ~/.ssh/known_hosts, а затем записываем снова с публичным ключом cat host_ca.pub > ~/.ssh/known_hosts. Теперь отредактируем nano ~/.ssh/known_hosts и добавим в начало @cert-authority *. 

~/.ssh/known_hosts

Теперь создаём ключи для пользователей аналогично как создавали ключи для хостов:
ssh-keygen -f user-key -b 4096 -t rsa
ssh-keygen -s user_ca -I <имя сертификата> -n <имена пользователей через запятую> -V +<время> user-key.pub
Пример создания ключей и сертификата для пользователей admin, branch_admin и network_admin (стоит упомянуть, что Linux чувствителен к регистру и возможно придется создать 2 сертификата под разных пользователей, так как пользователей LDAP мы не писали с заглавной буквы)
Создание сертификата для пользователей

Далее нужно настроить удаленную машину, чтобы она доверяла любому, кто представляет сертификат, выданный центром сертификации пользователя, при подключении. Для этого копируем файл scp user_ca.pub root@<ip_адрес>:/etc/ssh и редактируем файл на BR-SRV nano /etc/ssh/sshd_config и добавляем строчку:
TrustedUserCAKeys /etc/ssh/user_ca.pub

/etc/ssh/sshd_config
Теперь можно перезагрузить службу OpenSSH systemctl restart sshd.
Выдача сертификатов SSH на этом закончена.
a.	Выдайте сертификаты для веб серверов
3. Настройте SSH на всех Linux хостах: 
a.	Banner ( Authorized access only! ); 
b.	Установите запрет на доступ root; 
c.	Отключите аутентификацию по паролю; 
d.	Переведите на нестандартный порт; 
e.	Ограничьте ввод попыток до 4; 
f.	Отключите пустые пароли; 
g.	Установите предел времени аутентификации до 5 минут; 
h.	Установите авторизацию по сертификату выданным HQ-SRV 
Для этого создаём файл nano /etc/ssh/sshd-banner и прописываем строчку Authorized access only!
 
/etc/ssh/sshd-banner
И после этого вносим следующую строчку в nano /etc/ssh/sshd_config:
Banner /etc/ssh/sshd-banner

 /etc/ssh/sshd_config
a.	Установите запрет на доступ root; c. Отключите аутентификацию по паролю; 
Добавляем следующие строчки в nano /etc/ssh/sshd_config:
PermitRootLogin no
PasswordAuthentication no
 
/etc/ssh/sshd_config
 
/etc/ssh/sshd_config
b.	Переведите на нестандартный порт; 
Добавляем следующую строчку в nano /etc/ssh/sshd_config:
Port <номер_порта>
 
/etc/ssh/sshd_config
c.	Ограничьте ввод попыток до 4; 
Добавляем следующую строчку в nano /etc/ssh/sshd_config:
MaxAuthTries 4
 
/etc/ssh/sshd_config
d.	Отключите пустые пароли; 
Добавляем следующую строчку в nano /etc/ssh/sshd_config:
PermitEmptyPasswords no
 
/etc/ssh/sshd_config
e.	Установите предел времени аутентификации до 5 минут;
Добавляем следующую строчку в nano /etc/ssh/sshd_config:
LoginGraceTime 5m
Конфигурация /etc/ssh/sshd_config

f.	Установите авторизацию по сертификату выданным HQ-SRV
Данную настройку мы произвели во 2 задании 3 модуля. Проверить настройку можно командой ssh -i <путь_до_ключа_пользователя> -p <номер_порта> <имя_пользователя>@<FQDN_сервера>
Пример входа пользователя admin на BR-SRV с HQ-SRV по протоколу SSH
Успешный вход пользователя admin на BR-SRV

4. Реализуйте антивирусную защиту по средствам ClamAV на устройствах HQ-SRV и BR-SRV: 
a.	Настройте сканирование системы раз в сутки с сохранением отчета 
i. Учтите, что сканирование должно проводится при условии, что от пользователей нет нагрузки 
5. Настройте систему управления трафиком на роутере BR-R для контроля входящего трафика в соответствии со следующими правилами: 
a. Разрешите подключения к портам DNS (порт 53), HTTP (порт 80) и HTTPS (порт 443) для всех клиентов. Эти порты необходимы для работы настраиваемых служб. 
b. Разрешите работу выбранного протокола организации защищенной связи. 
c.Разрешение портов должно быть выполнено по принципу "необходимо и достаточно". 
d. Разрешите работу протоколов ICMP (протокол управления сообщениями Internet). 
e. Разрешите работу протокола SSH (Secure Shell) (SSH используется для безопасного удаленного доступа и управления устройствами). 
f. Запретите все прочие подключения. 
g.Все другие подключения должны быть запрещены для обеспечения безопасности сети. 

– разрешает работу DNS (порт 53, зарегистрированный IANA); 
– разрешает работу HTTP (порт 80, зарегистрированный IANA); 
– разрешает работу HTTPS (порт 443, зарегистрированный IANA); 
– отслеживание состояния соединений; 
– разрешает работу GRE; 
– разрешает работу ICMP; 
– разрешает работу IPSec (порт 500, зарегистрированный IANA); 
– разрешает обращение клиентов из офиса Left; 
– разрешает обращение клиентов из офиса Right; 
– запрещает весь прочий трафик по протоколу IPv4.
После написания конфигурации необходимо применить правила при помощи команды nft -f /etc/nftables.conf. Также можно использовать команду nft list ruleset, чтобы отобразить список всех текущих настроек.
6. Настройте виртуальный принтер с помощью CUPS для возможности печати документов из Linux-системы на сервере BR-SRV. 
 
7. Между офисами HQ и BRANCH установите защищенный туннель, позволяющий осуществлять связь между регионами с применением внутренних адресов. 
файл /etc/gre.up
После того, как скрипты были написаны, необходимо дать им права на выполнение при помощи команды chmod +x /etc/gre.up	для проверки на ошибки пропишите /etc/gre.up

Для придания защищенности apt install strongswan
 

После того, как конфигурация написана, необходимо добавить в автозагрузку и запустить IPSec при помощи команды 
systemctl enable --now ipsec
 
8. По средствам уже настроенного мониторинга установите следующие параметры: 
a.	Warning 
i. Нагрузка процессора больше или равна 70% 
ii. Заполненность оперативной памяти больше или равна 80% 
iii. Заполненность диска больше или равна 85% 
b. Напишите план действия при получении Warning сообщений 

9. Настройте программный RAID 5 из дисков по 1 Гб, которые подключены к машине BR-SRV. 
В рамках выполнения задания нам необходимо настроить RAID «Зеркало» или RAID1. Это делается при помощи утилиты mdadm, которую необходимо установить при помощи команды apt install mdadm 
Дальше создаем 2 жестких диска по 2 ГБ. 
После завершения настроек следует перезагрузить виртуальную машину, а после загрузки ввести команду lsblk.

Стоить обратить внимание на два диска объемом по 2 Гб — sda и sdc, они без разметки диска, поэтому не имеют цифрового идентификатора, в отличие от диска sdb, который имеет подразделы: – sdb1; – sdb2; – sdb5. Все разделы, перечисленные выше, — системные, в столбце MOUNTPOINT указано, в какую директорию смонтированы данные разделы диска. 
На новых неразмеченных дисках необходимо создать разделы при помощи утилиты cfdisk cfdisk /dev/sdX , где Х — буква того диска, который вы желаете отформатировать. Например, разметка диска /dev/sda будет выглядеть вот так: cfdisk /dev/sda — введите данную команду в терминал. Откроется псевдографическая утилита, на данном этапе необходимо указать метку раздела — значение по умолчанию, GPT.

Далее необходимо выбрать New для создания нового раздела. 

Далее указывается размер создаваемого раздела, по умолчанию указывается все свободное пространство целевого диска. Нажатием Enter происходит соглашение с размером раздела.
После чего необходимо перейти в опцию меню Type, чтобы указать, какой формат разметки выбрать. По умолчанию выбирается Linux Filesystem, но для создания RAID-массива потребуется отформатировать диск в Linux RAID.

В указанном списке выбирается Linux RAID. 


После внесенных правок необходимо покинуть утилиту cfdisk, выбрав параметр Quit
Необходимо повторить данную процедуру также и со вторым добавленным диском. 
Как проверить Созданные разделы можно увидеть в выводе команды lsblk.
Далее необходимо собрать RAID1-массив. Для этого необходимо воспользоваться командой mdadm --create /dev/md0 –level=1 --raid-devices=2 /dev/sdX1 /dev/sdX1 (X-ваши буквы)
После этого утилита mdadm начнет создание RAID-массива. Процесс сборки RAID-массива можно наблюдать в выводе файла /proc/mdstat cat /proc/mdstat

Обозначение букв UU (use;use) указывает на то, что оба диска прошли инициализацию и сборку в RAID-массив и готовы к работе. После того, как массив собран, его необходимо сохранить при помощи команды 
mdadm --detail --scan --verbose | tee -a /etc/mdadm/mdadm.conf
Далее необходимо пересоздать initramfs с поддержкой данного массива при помощи команды update-initramfs-u — данная команда обновит информацию о монтируемых при загрузке разделах RAID.  
Готовый массив также отобразится в выводе утилиты lsblk.
После того, как массив собран, необходимо форматировать его в файловую систему ext4 и настроить автоматическое монтирование при загрузке. Форматирование можно произвести при помощи команды mkfs.ext4 /dev/md0 Данная команда отформатирует файловый раздел в файловую систему ext4.
Далее необходимо создать точку монтирования данного массива, сделать это можно при помощи команды mkdir /mnt/storage
После этого необходимо добавить запись в файл /etc/fstab.

Протестировать автоматическое монтирование без перезагрузки машины можно при помощи команды mount –av.
После этого можно протестировать автоматическое монтирование перезагрузкой машины. Необходимо перезагрузить машину с помощью команды reboot. После успешной перезагрузки вводится команда lsblk.

10. Настройте Bacula на сервере HQ-SRV для резервного копирования etc на сервере BR-SRV 
Также необходимо включить IP-переадресацию на HQ-R, ISP и BR-R путём создания скрипта, который будет отрабатывать каждый раз при включении любого интерфейса и менять системную переменную net.ipv4.ip_forward на 1. Грубо говоря делается это для того, чтобы после перезагрузки устройства у вас не слетало значение переменной (после перезагрузки она возращается в 0). Cпособа действеннее и проще сделать это я не нашёл.
Создаём директорию mkdir /opt/scripts и в ней создаём файл nano /opt/scripts/ipforward.sh и вписываем туда следующие строчки:
#!/bin/sh
sysctl -w net.ipv4.ip_forward=1
После этого выдаём права на запуск:
chmod +x /opt/scripts/ipforward.sh.
Затем создаём файл службы, которая будет исполнять скрипт при запуске системы nano /etc/systemd/system/ipforward.service и прописываем в него следующие строки (рис. 6):
[Unit]
Description=Run a Custom Script at Startup
After=default.target
[Service]
ExecStart=/opt/scripts/ipforward.sh
[Install]
WantedBy=default.target
 /etc/system/system/ipforward.sh 
Теперь надо обновить файлы конфигурации systemd и включить службу:
systemctl daemon-reload
systemctl enable ipforward.service
В конце концов пропишите sysctl -w net.ipv4.ip_forward=1  на всех устройствах самостоятельно, потому что скрипт отработает только при перезагрузке системы.
7. Настройте подключение по SSH для удалённого конфигурирования устройства HQ-SRV по порту 2222. Учтите, что вам необходимо перенаправить трафик на этот порт по средствам контролирования трафика.
Для начала установим на HQ-SRV OpenSSH сервер apt install openssh-server и для простой проверки разрешим удалённое подключение учётной записи root. nano /etc/ssh/sshd_config и PermitRootLogin yes (рис. 15), затем перезапускаем OpenSSH сервер при помощи systemctl restart sshd.service.

/etc/ssh/sshd_config
Выполним перенаправление трафика при помощи встроенной утилиты nftables на Debian. Будем перенаправлять трафик, который будет поступать на внешний интерфейс HQ-R на порт 2222 на 22 порт сервера HQ-SRV:
nft add table ip nat # Создаём таблицу, работающую с IP с названием nat
nft add chain ip nat prerouting { type nat hook prerouting priority dstnat \; policy accept \; } # Создаём цепочку prerouting внутри таблицы nat с политикой принимать любые поступающие пакеты
nft add rule ip nat prerouting iifname "<имя_интерфейса>" tcp dport 2222 dnat to <ip_адрес_сервера>:22  # Добавляем правило на перенаправление портов
nft add chain ip nat postrouting { type nat hook postrouting priority srcnat \; policy accept \; } # Аналогично создаём postrouting внутри nat
nft add rule ip nat postrouting oifname "<имя_интерфейса>" masquerade # Добавление правила на трансляцию выходящих IP-адресов
nft list ruleset > /etc/nftables.conf # Сохраняем изменения для последующих перезагрузок устройства
systemctl start nftables.service # Запускаем nftables
systemctl enable nftables.service # Устанавливаем что служба будет запускаться вместе с устройством
Пример настройки на рис. 16.
 
Конфигурация nftables
Проверить можно написав команду с любого внешнего устройства ssh -p 2222 4.4.4.100
